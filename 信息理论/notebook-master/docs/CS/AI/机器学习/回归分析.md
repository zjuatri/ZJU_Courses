# 回归分析

起源：“衰退”(regression)现象

## 线性回归

### 一元线性模型

!!! note ""

    模型：$y=ax+b$

    本质：寻找直线，使得尽可能靠近数据点，以最小误差进行拟合

    损失：残差平方和$\dfrac{1}{N}\sum(y-\tilde y)^2$最小，残差即预测值和真实值之间的差值

    参数求解：最小二乘法

优化目标：

$$
\min_{a,b}L(a,b)=\sum_{i=1}^n(y_i-ax_i-b)^2
$$

对b求偏导：

$$
\dfrac{\partial L(a,b)}{\partial b}=0\Rightarrow\sum_{i=1}^n(y_i-ax_i-b)=0
$$

$$
\therefore b=\bar y-a\bar x
$$

对a求偏导：

$$
\dfrac{\partial L(a,b)}{\partial a}=0\Rightarrow a=\dfrac{\sum\limits_{i=1}^nx_iy_i-n\bar x\bar y}{\sum\limits_{i=1}^nx_i^2-n\bar x^2}$$

### 多元线性模型

推广：多维

目的：找到一组参数 $\mathbf a$，使得线性函数 $f(\mathbf x)=a_0+\mathbf a^{\mathsf T}\mathbf x$ 尽可能拟合数据，即最小化均方误差函数：$J_m=\frac{1}{m}\sum_{i=1}^m(y_i-f(\mathbf x_i))^2$ 。我们将每一个数据 $𝑥_𝑖$ 扩展一个维度，其值为 $1$ ，对应参数 $𝑎_0$，则有

$$J_m=\frac{1}{m}(\mathbf{y}-\mathbf{X}^\top\mathbf{a})^\top(\mathbf{y}-\mathbf{X}^\top\mathbf{a})$$

- 均方误差函数对 $\mathbf{a}$ 求导得 

$$
\begin{aligned}
\nabla_\mathbf{a} J_m &=\frac{2}{m}(\nabla_\mathbf{a}(\mathbf{y}-\mathbf{X}^\top\mathbf{a})^\top)(\mathbf{y}-\mathbf{X}^\top\mathbf{a})\\&=\frac{2}{m}(\nabla_\mathbf{a}(\mathbf{y}^T-\mathbf{a}^T\mathbf{X}))(\mathbf{y}-\mathbf{X}^\top\mathbf{a})\\&=-\frac{2}{m}\mathbf{X}(\mathbf{y}-\mathbf{X}^\top\mathbf{a})
\end{aligned}
$$

令其为 0 解得 $\mathbf{a}=(\mathbf{X}\mathbf{X}^\top)^{-1}\mathbf{X}\mathbf{y}$

## 非线性回归

- 逻辑斯蒂回归/对数几率回归
    - 线性回归对离群点非常敏感，导致模型不稳定，为了缓解这个问题可以考虑逻辑斯蒂回归（logistic regression）
    - 在回归模型中引入 sigmoid 函数，逻辑斯蒂回归模型

    $$
    y=\dfrac{1}{1+e^{-z}}=\dfrac{1}{1+e^{\mathbf{w}^\top\mathbf{x}+b}}
    $$

    - 逻辑斯蒂回归函数的输出具有概率意义，一般用于二分类问题
    - 逻辑斯蒂回归是一个线性模型，在预测时可以计算线性函数 $\mathbf{w}^\top\mathbf{x}+b$ 取值是否大于 0 来判断输入数据的类别归属
    - 求解参数的典型做法是最大化对数似然（log likelihood）函数
    - 最大似然估计目的是计算似然函数的最大值，而分类过程是需要损失函数最小化，常用梯度下降法（gradient descent）：批量梯度下降、随机梯度下降、小批量梯度下降
    - 只能用于解决二分类问题
    - 多分类可以将其推广为多项逻辑斯蒂回归模型，即 softmax 函数