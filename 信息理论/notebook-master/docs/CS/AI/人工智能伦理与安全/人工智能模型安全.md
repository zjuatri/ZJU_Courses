对抗攻击：在输入识别样本中人为故意添加若干人类无法察觉的细微干扰信息，导致模型以高置信度给出错误的识别结果，这一攻击人工智能模型的行为被称为对抗攻击

对抗样本生成：$$
\text{Minimize}\Vert\delta\Vert_2\quad s.t.f(x+\delta)=y',x+\delta\in[0,1]^m
$$
L-BFGS：$$
\text{Minimize}\ c|\delta|+L_{CE}(x+\delta,y')\quad s.t.x+\delta\in[0,1]^m
$$
其中$L_{CE}$为交叉熵损失函数

FSGM：$$
x'=x+\delta=x+\eta\cdot\text{sign}(\nabla_xf(x))
$$
PGD：$$
x'_{k+1}=\text{Proj}\set{x_k'+\alpha\cdot\text{sign}(\nabla_xf(x_k'))}
$$
黑盒攻击：攻击者可以自己训练一个替代网络，利用可访问梯度的替代模型生成对抗样本，并借助对抗样本在不同模型的迁移性实现对原始模型的攻击

数据投毒：

+ 标签反转投毒
+ 干净样本投毒攻击
+ 后门攻击：增加后门触发器

防御：

+ 测试阶段防御
+ 训练阶段防御

隐私保护：

+ 差分隐私
+ 同态加密
+ 安全多方计算

